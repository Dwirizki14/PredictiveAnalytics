# -*- coding: utf-8 -*-
"""Predictive_Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z3BXKyqcesKldw97w-FQ0-nJLouRUYV6

#**Laporan Proyek Machine Learning - Dwi Rizki Kurnia**#

# **Domain Proyek**

Biaya asuransi kesehatan merupakan isu penting dalam sistem pelayanan medis, baik di negara maju maupun berkembang. Perusahaan asuransi berusaha menyesuaikan premi berdasarkan risiko individu, yang biasanya ditentukan oleh faktor-faktor seperti usia, status merokok, BMI, jumlah anak, dan wilayah tempat tinggal.

Namun, menghitung biaya asuransi secara manual seringkali tidak efisien dan rawan bias. Oleh karena itu, penggunaan machine learning dapat membantu dalam memprediksi biaya asuransi secara objektif dan akurat berdasarkan data historis pelanggan.

Referensi:

- Chen, C., et al. "Medical Cost Personal Dataset." Kaggle, 2018.  
- Xu, R., & Wunsch, D. "Clustering algorithms in biomedical research: A review." IEEE Reviews in Biomedical Engineering, 2010.

# **Business Understanding**

**Problem Statements**

1. Bagaimana memprediksi biaya asuransi medis berdasarkan karakteristik pelanggan?

2. Fitur apa saja yang paling memengaruhi besarnya biaya asuransi?

3. Bagaimana membangun model prediksi yang akurat dan dapat diandalkan untuk pelanggan baru?

**Goals**

1. Membangun model prediktif yang mampu menghasilkan prediksi biaya asuransi yang akurat dan dapat dipercaya.

2. Membangun model prediktif untuk memperkirakan biaya asuransi pelanggan baru.

3. Menyediakan solusi machine learning yang optimal untuk diaplikasikan pada data pelanggan baru, sehingga memudahkan penetapan premi yang sesuai.

**Solution Statements**

1. Membangun model baseline menggunakan Linear Regression.

2. Melakukan peningkatan model dengan menggunakan algoritma seperti Random Forest Regressor dan XGBoost.

3. Melakukan evaluasi dan perbandingan model menggunakan metrik MAE, MSE, dan RMSE untuk memilih model terbaik.

# **Data Understanding**

Dataset ini berjudul "Medical Cost Personal Dataset" dan tersedia di Kaggle:
ðŸ”— https://www.kaggle.com/datasets/mirichoi0218/insurance

Dataset berisi 1338 baris dan 7 fitur.

Fitur-Fitur:

- age	(Usia pelanggan) (integer)

- sex	(Jenis kelamin) (male/female)

- bmi	(Body Mass Index) (float)

- children (Jumlah anak tanggungan) (integer)

- smoker	(Status merokok) (yes/no)

- region	(Wilayah tinggal) (northeast, northwest, southeast, southwest)

- charges	(Biaya asuransi yang dibayarkan) (float) â€” target variabel

# Data Loading
"""

# Commented out IPython magic to ensure Python compatibility.
#Import Library
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download mirichoi0218/insurance

!unzip insurance.zip

"""# Exploratory Data Analysis - Deskripsi Variabel"""

data = pd.read_csv("insurance.csv")

# Descriptive Statistics
print(data.info())

print(data.describe())

print(data.head())

"""# Menangani Missing Value dan Outliers"""

# Cek missing values
missing_values = data.isnull().sum()
print(missing_values)

"""Pada dataset ini, saya memeriksa apakah ada nilai yang hilang (missing values). Setelah pengecekan, ditemukan bahwa tidak ada missing value pada fitur-fitur utama. Hal ini memudahkan proses modeling karena tidak perlu imputasi data."""

# Cek outliers menggunakan IQR untuk kolom numerik
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
outliers = {}
for col in numerical_columns:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    outliers[col] = data[(data[col] < Q1 - 1.5 * IQR) | (data[col] > Q3 + 1.5 * IQR)].shape[0]
    print(f"Outliers in {col}: {outliers[col]}")

numerical_features = ['age', 'bmi', 'children', 'charges']

for feature in numerical_features:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=data[feature])
    plt.title(f'Boxplot of {feature}')
    plt.show()

Q1 = data[numerical_features].quantile(0.25)
Q3 = data[numerical_features].quantile(0.75)
IQR = Q3 - Q1

data_clean = data[~((data[numerical_features] < (Q1 - 1.5 * IQR)) | (data[numerical_features] > (Q3 + 1.5 * IQR))).any(axis=1)]

"""# Univariate Analysis

**Bagi fitur pada dataset menjadi dua bagian**
"""

# Fitur numerik
numerical_features = ['age', 'bmi', 'children', 'charges']

# Fitur kategorikal
categorical_features = ['sex', 'smoker', 'region']

"""#Numerical Features#"""

for feature in numerical_features:
    plt.figure(figsize=(8, 4))
    sns.histplot(data_clean[feature], kde=True)
    plt.title(f'Distribution of {feature}')
    plt.show()
    print(data_clean[feature].describe())

"""# Categorical Features"""

categorical_features = ['sex', 'smoker', 'region']

for feature in categorical_features:
    plt.figure(figsize=(8, 4))
    sns.countplot(x=data_clean[feature])
    plt.title(f'Count of {feature}')
    plt.show()
    print(data_clean[feature].value_counts(normalize=True) * 100)

"""**Sex**"""

feature = 'sex'
count = data[feature].value_counts()
percent = 100 * data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel': count, 'persentase (%)': percent.round(1)})
print(df)

# Visualisasi
count.plot(kind='bar', title='Distribusi Kategori: Sex', color='lightcoral', edgecolor='black')
plt.ylabel('Jumlah Sampel')
plt.xlabel('Kategori')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""**Smoker**"""

feature = 'smoker'
count = data[feature].value_counts()
percent = 100 * data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel': count, 'persentase (%)': percent.round(1)})
print(df)

# Visualisasi
count.plot(kind='bar', title='Distribusi Kategori: Smoker', color='gold', edgecolor='black')
plt.ylabel('Jumlah Sampel')
plt.xlabel('Kategori')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""**region**"""

feature = 'region'
count = data[feature].value_counts()
percent = 100 * data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel': count, 'persentase (%)': percent.round(1)})
print(df)

# Visualisasi
count.plot(kind='bar', title='Distribusi Kategori: Region', color='mediumseagreen', edgecolor='black')
plt.ylabel('Jumlah Sampel')
plt.xlabel('Kategori')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""# Multivariate Analysis"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(data, diag_kind = 'kde')

"""Korelasi Antar Fitur Numerik"""

# Korelasi
plt.figure(figsize=(8, 6))
sns.heatmap(data[numerical_features].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Korelasi antar fitur numerik")
plt.tight_layout()
plt.show()

"""# Data Preparation

**Encoding Fitur Kategori**
"""

data_encoded = pd.get_dummies(data_clean, columns=categorical_features, drop_first=True)

"""Pada dataset ini, beberapa fitur bersifat kategori seperti sex, smoker, dan region. Karena algoritma machine learning umumnya hanya dapat memproses data numerik, maka fitur kategori diubah menjadi representasi numerik menggunakan teknik one-hot encoding. Misalnya, fitur sex diubah menjadi sex_male dan sex_female dengan nilai 0/1

**Reduksi Dimensi dengan PCA**
"""

# Pisahkan fitur dan target
X = data_encoded.drop('charges', axis=1)
y = data_encoded['charges']

# Standarisasi sebelum PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA (jumlah komponen = jumlah fitur)
pca = PCA()
X_pca = pca.fit_transform(X_scaled)


plt.figure(figsize=(8,5))
plt.plot(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_.cumsum(), marker='o')
plt.title("Kumulatif Varians oleh Komponen PCA")
plt.xlabel("Jumlah Komponen")
plt.ylabel("Kumulatif Varians")
plt.grid(True)
plt.show()

"""Reduksi dimensi membantu meningkatkan efisiensi komputasi dan dapat meningkatkan performa model dengan mengurangi noise dan fitur yang kurang informatif. PCA juga membantu mengurangi risiko overfitting.

**train_test_split**
"""

X = data_encoded.drop('charges', axis=1)
y = data_encoded['charges']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Standarisasi Fitur**"""

numerical_features_for_scaling = [feature for feature in numerical_features if feature != 'charges']

scaler = StandardScaler()
X_train[numerical_features_for_scaling] = scaler.fit_transform(X_train[numerical_features_for_scaling])
X_test[numerical_features_for_scaling] = scaler.transform(X_test[numerical_features_for_scaling])

"""Fitur numerik seperti age, bmi, dan charges distandarisasi menggunakan teknik StandardScaler agar memiliki mean = 0 dan standar deviasi = 1

# Modeling
"""

models = {
    'KNN': KNeighborsRegressor(),
    'Random Forest': RandomForestRegressor(random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42)
}

for name, model in models.items():
    model.fit(X_train, y_train)

"""Saya menggunakan tiga algoritma regresi yaitu K-Nearest Neighbor (KNN), Random Forest, dan Gradient Boosting. Pemilihan ini dilakukan untuk membandingkan performa model berbasis instance-based (KNN), ensemble decision tree (Random Forest), dan boosting (Gradient Boosting).


"""

y_pred_knn = models['KNN'].predict(X_test)
y_pred_rf = models['Random Forest'].predict(X_test)
y_pred_gb = models['Gradient Boosting'].predict(X_test)

print(len(y_pred_knn), len(y_pred_rf), len(y_pred_gb))

"""# Evaluasi Model"""

for name, model in models.items():
    predictions = model.predict(X_test)
    print(f'{name} Evaluation:')
    print('MAE:', mean_absolute_error(y_test, predictions))
    print('MSE:', mean_squared_error(y_test, predictions))
    print('RMSE:', np.sqrt(mean_squared_error(y_test, predictions)))
    print('RÂ²:', r2_score(y_test, predictions))
    print('\n')

"""Dari data di atas, dapat disimpulkan bahwa model Gradient Boosting memberikan hasil terbaik dengan MAE dan RMSE terkecil serta nilai RÂ² terbesar (0.63). Artinya, model ini mampu memprediksi biaya asuransi dengan kesalahan rata-rata yang lebih kecil dan menjelaskan 63% variasi biaya asuransi berdasarkan fitur yang tersedia.

Metrik yang dipakai

Saya menggunakan MAE (Mean Absolute Error), MSE (Mean Squared Error), RMSE (Root Mean Squared Error), dan RÂ² (koefisien determinasi).

Penjelasan metrik

- MAE mengukur rata-rata kesalahan absolut antara prediksi dan nilai aktual, lebih mudah diinterpretasikan secara langsung (satuan sama dengan target).

- MSE memberikan penalti lebih besar terhadap kesalahan besar karena kuadrat dari error.

- RMSE adalah akar dari MSE, membuat skala kesalahan sama dengan target sehingga lebih mudah dibandingkan.

- RÂ² menunjukkan proporsi variansi target yang dapat dijelaskan oleh model, semakin mendekati 1 semakin baik.

Alasan dipakai
Metrik ini cocok untuk regresi dan memberikan gambaran lengkap dari kesalahan prediksi dan kemampuan model dalam menjelaskan data.
"""

data_prediksi = pd.DataFrame({
    'y_true': y_test,
    'prediksi_knn': y_pred_knn,
    'prediksi_rf': y_pred_rf,
    'prediksi_gb': y_pred_gb
})

print(data_prediksi.head())

"""**Metrik Evaluasi yang Digunakan**

Dalam proyek prediksi biaya asuransi ini, metrik evaluasi yang digunakan meliputi:

* Mean Absolute Error (MAE)
MAE mengukur rata-rata absolut selisih antara nilai aktual dan nilai prediksi. MAE mudah diinterpretasikan karena satuannya sama dengan data asli (dalam kasus ini biaya asuransi dalam satuan mata uang). MAE memberikan gambaran seberapa jauh prediksi model rata-rata menyimpang dari nilai asli tanpa memperhatikan arah kesalahan.

*   Mean Squared Error (MSE)
MSE mengkuadratkan selisih antara nilai aktual dan prediksi, sehingga penalti terhadap kesalahan besar menjadi lebih berat. MSE cocok digunakan untuk mendeteksi prediksi yang sangat jauh dari nilai sebenarnya.


*   Root Mean Squared Error (RMSE)
RMSE adalah akar dari MSE, yang mengembalikan skala kesalahan ke satuan asli data sehingga lebih mudah diinterpretasikan dibandingkan MSE. RMSE juga sangat sensitif terhadap outlier.

* R-squared (RÂ²)
RÂ² mengukur proporsi variansi target yang dapat dijelaskan oleh model. Nilai RÂ² berkisar dari 0 sampai 1, di mana nilai mendekati 1 menunjukkan model sangat baik dalam menjelaskan variasi data.


**Kesimpulan**

Metrik evaluasi yang dipilih sudah sesuai dengan tujuan proyek yaitu menghasilkan prediksi biaya asuransi yang akurat dan dapat dipercaya. Model Gradient Boosting layak dipilih sebagai model final karena memberikan keseimbangan terbaik antara kesalahan prediksi dan kemampuan menjelaskan variansi data.
"""